{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MA691 Project",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM7LRVF5YFMm"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fezpNdMla2f8",
        "outputId": "65b9e188-32c8-4ca9-c528-6f0e677fdba8"
      },
      "source": [
        "!pip install pycobra"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycobra\n",
            "  Downloading pycobra-0.2.5-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from pycobra) (0.11.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pycobra) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pycobra) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pycobra) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pycobra) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pycobra) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycobra) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycobra) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycobra) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycobra) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->pycobra) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->pycobra) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pycobra) (1.0.1)\n",
            "Installing collected packages: pycobra\n",
            "Successfully installed pycobra-0.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_7CddPhAuSQ"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial \n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from pycobra.classifiercobra import ClassifierCobra"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-15r4FM10Zna"
      },
      "source": [
        "# Undersampling Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CRRCmp5gVZL"
      },
      "source": [
        "## Type - 1: Selecting instances of the Majority Class to keep in the final Training Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsDYB3nlsekE"
      },
      "source": [
        "### 1. Near Miss Alogrithm - v1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdnZkmtpA4fb"
      },
      "source": [
        "def near_miss_v1(X, y, majority_class, k = 3, ratio = 4.5, knn_algorithm = 'auto', knn_metric = 'euclidean'):\n",
        "  n, _ = X.shape\n",
        "  verdict = np.ones(n, dtype=bool)\n",
        "\n",
        "  # Step - 1: Find k nearest neighbors (in minority class) of all the data points\n",
        "  X_minority = []\n",
        "  for idx, val in enumerate(X):\n",
        "    if y[idx] != majority_class:\n",
        "      X_minority.append(X[idx])\n",
        "      \n",
        "  nearest_neighbors = NearestNeighbors(n_neighbors = k, algorithm = knn_algorithm, metric = knn_metric).fit(X_minority)\n",
        "  distances, nearest_neighbors_idx = nearest_neighbors.kneighbors(X)\n",
        "  \n",
        "  # Step - 2: Compute the average distance\n",
        "  distances = np.mean(distances, axis = 1)\n",
        "  \n",
        "  # Step - 3: Select the points from majority class depending on the 'ratio' value\n",
        "  X_majority_info = []  # stores a 2D value, where val at idx1 is the index in the original data and val at idx2 is the mean distance\n",
        "  for i in range(n):\n",
        "    class_label = y[i]\n",
        "    if class_label == majority_class:\n",
        "      X_majority_info.append([i, distances[i]])\n",
        "\n",
        "  majority_size = len(X_majority_info)\n",
        "  X_majority_info.sort(key=lambda x: x[1])\n",
        "  req_count = min(majority_size, (int)(ratio * len(X_minority)))\n",
        "  \n",
        "  print(\"[Testing]: Count of majority samples after undersampling vs Count of minority samples = \", req_count, \"vs\", len(X_minority), \"or\", req_count * 100/len(X_minority), \"%\")\n",
        "  for i in range(req_count, majority_size):\n",
        "    verdict[X_majority_info[i][0]] = False  \n",
        "\n",
        "  return verdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tvt80QIGsirC"
      },
      "source": [
        "### 2. Near Miss Algorithm - v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRFQmVvYm1rZ"
      },
      "source": [
        "def near_miss_v2(X, y, majority_class, k = 3, ratio = 4.5, knn_algorithm = 'auto', knn_metric = 'euclidean'):\n",
        "  n, _ = X.shape\n",
        "  verdict = np.ones(n, dtype=bool)\n",
        "\n",
        "  # Step - 1: Find k farthest neighbors (in minority class) of all the data points\n",
        "  X_minority = []\n",
        "  for idx, val in enumerate(X):\n",
        "    if y[idx] != majority_class:\n",
        "      X_minority.append(X[idx])\n",
        "      \n",
        "  distances = []\n",
        "  for idx, maj_val in enumerate(X):\n",
        "    if idx % 500 == 0:\n",
        "      print(\"[Executing]: Processing data point - {} while undersampling...\".format(idx + 1))\n",
        "\n",
        "    if y[idx] != majority_class:\n",
        "      distances.append([0] * k)    # To allow executing 'np.mean(distances, axis = 1)'\n",
        "    else:\n",
        "      current_distances = []\n",
        "      for min_val in X_minority:\n",
        "        current_distances.append(np.linalg.norm(maj_val - min_val))\n",
        "      current_distances.sort(reverse = True)\n",
        "\n",
        "      distances.append(current_distances[: min(k, len(X_minority))])\n",
        "  \n",
        "  # Step - 2: Compute the average distance\n",
        "  distances = np.mean(distances, axis = 1)\n",
        "  \n",
        "  # Step - 3: Select the points from majority class depending on the 'ratio' value\n",
        "  X_majority_info = []  # stores a 2D value, where val at idx1 is the index in the original data and val at idx2 is the mean distance\n",
        "  for i in range(n):\n",
        "    class_label = y[i]\n",
        "    if class_label == majority_class:\n",
        "      X_majority_info.append([i, distances[i]])\n",
        "\n",
        "  majority_size = len(X_majority_info)\n",
        "  X_majority_info.sort(key=lambda x: x[1])\n",
        "  req_count = min(majority_size, (int)(ratio * len(X_minority)))\n",
        "  \n",
        "  print(\"[Testing]: Count of majority samples after undersampling vs Count of minority samples = \", req_count, \"vs\", len(X_minority), \"or\", req_count * 100/len(X_minority), \"%\")\n",
        "  for i in range(req_count, majority_size):\n",
        "    verdict[X_majority_info[i][0]] = False  \n",
        "\n",
        "  return verdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSuNXcCfslj6"
      },
      "source": [
        "### 3. Near Miss Algorithm - v3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xcufqj8tCS5"
      },
      "source": [
        "def near_miss_v3(X, y, majority_class, ratio = 3.5, knn_algorithm = 'auto', knn_metric = 'euclidean'):\n",
        "  n, _ = X.shape\n",
        "  verdict = np.ones(n, dtype=bool)\n",
        "\n",
        "  # Step - 1: Find k farthest neighbors (in minority class) of all the data points\n",
        "  X_minority = []\n",
        "  for idx, val in enumerate(X):\n",
        "    if y[idx] != majority_class:\n",
        "      X_minority.append(X[idx])\n",
        "      \n",
        "  distances = []\n",
        "  for idx, maj_val in enumerate(X):\n",
        "    if idx % 500 == 0:\n",
        "      print(\"[Executing]: Processing data point - {} while undersampling...\".format(idx + 1))\n",
        "\n",
        "    if y[idx] != majority_class:\n",
        "      distances.append([0] * len(X_minority))    # To allow executing 'np.mean(distances, axis = 1)'\n",
        "    else:\n",
        "      current_distances = []\n",
        "      for min_val in X_minority:\n",
        "        current_distances.append(np.linalg.norm(maj_val - min_val))\n",
        "      current_distances.sort(reverse = True)\n",
        "\n",
        "      distances.append(current_distances)\n",
        "  \n",
        "  # Step - 2: Compute the average distance\n",
        "  distances = np.mean(distances, axis = 1)\n",
        "  \n",
        "  # Step - 3: Select the points from majority class depending on the 'ratio' value\n",
        "  X_majority_info = []  # stores a 2D value, where val at idx1 is the index in the original data and val at idx2 is the mean distance\n",
        "  for i in range(n):\n",
        "    class_label = y[i]\n",
        "    if class_label == majority_class:\n",
        "      X_majority_info.append([i, distances[i]])\n",
        "\n",
        "  majority_size = len(X_majority_info)\n",
        "  X_majority_info.sort(key=lambda x: x[1])\n",
        "  req_count = min(majority_size, (int)(ratio * len(X_minority)))\n",
        "  \n",
        "  print(\"[Testing]: Count of majority samples after undersampling vs Count of minority samples = \", req_count, \"vs\", len(X_minority), \"or\", req_count * 100/len(X_minority), \"%\")\n",
        "  for i in range(req_count, majority_size):\n",
        "    verdict[X_majority_info[i][0]] = False  \n",
        "\n",
        "  return verdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRH9Nf6d9Wwi"
      },
      "source": [
        "### 4. Condensed KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wVzsszZ9bpW"
      },
      "source": [
        "def condensed_knn(X, y, majority_class, knn_algorithm = 'auto', knn_metric = 'euclidean'):\n",
        "  n, _ = X.shape\n",
        "  verdict = np.zeros(n, dtype=bool)\n",
        "\n",
        "  # Step - 1: Find k nearest neighbors (in minority class) of all the data points\n",
        "  store, store_y, grabstore, grabstore_y, s_idx, g_idx = [], [], [], [], [], []\n",
        "  X_minority, X_majority = [], []\n",
        "\n",
        "  for idx, val in enumerate(X):\n",
        "    if y[idx] != majority_class:\n",
        "      X_minority.append(X[idx])\n",
        "      s_idx.append(idx)\n",
        "      store.append(X[idx])\n",
        "      store_y.append(y[idx])\n",
        "    else:\n",
        "      X_majority.append(X[idx])\n",
        "      g_idx.append(idx)\n",
        "      grabstore.append(X[idx])\n",
        "      grabstore_y.append(y[idx])\n",
        "    \n",
        "  # Step - 2: Compute the average distance\n",
        "  cnt = 100\n",
        "  while((cnt != 0) and (len(grabstore)!=0)):\n",
        "    cnt = 0\n",
        "    i = 0\n",
        "    for sn, i in enumerate(np.array(grabstore)):\n",
        "      tmp = []\n",
        "      nearest_neighbors = NearestNeighbors(n_neighbors = 1, algorithm = knn_algorithm, metric = knn_metric).fit(store)\n",
        "      distances, nearest_neighbors_idx = nearest_neighbors.kneighbors(i.reshape(1,-1))\n",
        "      if(store_y[nearest_neighbors_idx[0][0]] != majority_class):\n",
        "        store.append(i)\n",
        "        store_y.append(grabstore_y[sn])\n",
        "        s_idx.append(g_idx[sn])\n",
        "        tmp.append(sn)\n",
        "        cnt += 1\n",
        "    for z in tmp:\n",
        "      del(grabstore[z])\n",
        "      del(grabstore_y[z])\n",
        "      del(g_idx[z])   \n",
        "  \n",
        "  for i in s_idx:\n",
        "    verdict[i] = True\n",
        "\n",
        "  return verdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYsEbvVkgxjZ"
      },
      "source": [
        "## Type - 2: Selecting instances of the Majority Class to delete from the final Training Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwYZzbrRsoma"
      },
      "source": [
        "### 1. KNN Und"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hp6Lo8QA7oL"
      },
      "source": [
        "def knn_und(X, y, majority_class, k = 50, t = 2, knn_algorithm = 'auto', knn_metric = 'euclidean'):\n",
        "  n, _ = X.shape\n",
        "  verdict = np.ones(n, dtype=bool)\n",
        "\n",
        "  # Step - 1: Find k nearest neighbors of all the data points \n",
        "  nearest_neighbors = NearestNeighbors(n_neighbors = k, algorithm = knn_algorithm, metric = knn_metric).fit(X)\n",
        "  _, nearest_neighbors_idx = nearest_neighbors.kneighbors(X)\n",
        "\n",
        "  for i in range(n):\n",
        "    # Step - 2: Identify the class label of X_i\n",
        "    class_label = y[i]\n",
        "\n",
        "    # Step - 3: Proceed only with majority class data points\n",
        "    if class_label != majority_class:\n",
        "      continue\n",
        "\n",
        "    # Step - 4: Identify the count of minority class neighbors for the X_i\n",
        "    minority_class_neighbors_count = 0\n",
        "    neighbors_class_label = y[nearest_neighbors_idx[i]]\n",
        "\n",
        "    for j in neighbors_class_label:\n",
        "      if j != majority_class:\n",
        "        minority_class_neighbors_count += 1\n",
        "\n",
        "    # Step - 5: Mark the data point as False if count of minority class neighbors >= t\n",
        "    if minority_class_neighbors_count >= t:\n",
        "      verdict[i] = False\n",
        "\n",
        "  return verdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laEHCFp6YQyh"
      },
      "source": [
        "###2. ENN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2uIqMjxYQPS"
      },
      "source": [
        "def enn(X, y, majority_class, k = 5, knn_algorithm = 'auto', knn_metric = 'euclidean'):\n",
        "  n, _ = X.shape\n",
        "  verdict = np.ones(n, dtype=bool)\n",
        "\n",
        "  # Step - 1: Find k nearest neighbors of all the data points \n",
        "  nearest_neighbors = NearestNeighbors(n_neighbors = k, algorithm = knn_algorithm, metric = knn_metric).fit(X)\n",
        "  _, nearest_neighbors_idx = nearest_neighbors.kneighbors(X)\n",
        "\n",
        "  # Step - 2: Delete elements from Majority Class\n",
        "  for i in range(n):\n",
        "    if(y[i] == majority_class):\n",
        "      tmp = 0\n",
        "      for j in nearest_neighbors_idx[i]:\n",
        "        tmp += y[j]\n",
        "      tmp/=3\n",
        "      if(tmp<0.6):\n",
        "        verdict[i] = False\n",
        "    else:\n",
        "      tmp = 0\n",
        "      for j in nearest_neighbors_idx[i]:\n",
        "        tmp += y[j]\n",
        "      tmp/=3\n",
        "      if(tmp>0.6):    \n",
        "        for j in nearest_neighbors_idx[i]:\n",
        "          if(y[j] == majority_class):\n",
        "            verdict[j] == False\n",
        "\n",
        "  return verdict            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dk_S-4yWmRn"
      },
      "source": [
        "### 3. Tomek Links"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8F3VicIWo94"
      },
      "source": [
        "def tomek_link(X, y, majority_class, knn_algorithm = 'auto', knn_metric = 'euclidean'):\n",
        "  n, _ = X.shape\n",
        "  verdict = np.ones(n, dtype=bool)\n",
        "\n",
        "  # Step - 1: Find k nearest neighbors of all the data points (k = 1)\n",
        "  nearest_neighbors = NearestNeighbors(n_neighbors = 1, algorithm = knn_algorithm, metric = knn_metric).fit(X)\n",
        "  _, nearest_neighbors_idx = nearest_neighbors.kneighbors(X)\n",
        "\n",
        "  for i in range(n):\n",
        "    # Step - 2: Identify the class label of X_i\n",
        "    class_label = y[i]\n",
        "\n",
        "    # Step - 3: Proceed only with majority class data points\n",
        "    if class_label != majority_class:\n",
        "      continue\n",
        "\n",
        "    # Step - 4: Remove the data point if it is part of a Tomek Link\n",
        "    neighbor_class_label = y[nearest_neighbors_idx[i][0]]\n",
        "    if neighbor_class_label != majority_class:\n",
        "      verdict[i] = False\n",
        "\n",
        "  return verdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aI_4iRzs3vC"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l91QKkXKfuuT"
      },
      "source": [
        "## Parent Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePASDCAYaA_F"
      },
      "source": [
        "def execute_model(X, y, num_splits, seed, model, with_undersampling = False, majority_class = 0, undersampling_method = knn_und):\n",
        "  K_folds = StratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n",
        "  metrics_list = []\n",
        "  \n",
        "  # Feature Scaling\n",
        "  sc = StandardScaler()\n",
        "  X = sc.fit_transform(X)\n",
        "\n",
        "  iterations = 1\n",
        "  for train_idx, test_idx in K_folds.split(X, y):\n",
        "    print(\"\\n****************  Executing iteration - {} of KFold Data split  ****************\".format(iterations))\n",
        "\n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = y[train_idx], y[test_idx]\n",
        "    \n",
        "    # Execute Undersampling on training data\n",
        "    if with_undersampling == True:\n",
        "      print(\"[Testing]: Count of test data before Undersampling = \", X_train.shape[0])\n",
        "      verdict = undersampling_method(X_train, y_train, majority_class)\n",
        "\n",
        "      X_train = X_train[verdict, :]\n",
        "      y_train = y_train[verdict]\n",
        "\n",
        "      # In-buit near miss algorithm\n",
        "      # nr = NearMiss()\n",
        "      # X_train, y_train = nr.fit_sample(X_train, y_train)\n",
        "\n",
        "      # Note: Be careful while plotting, make sure same features are being compared\n",
        "      # plt.scatter(X_train[:, 0], X_train[:, 1], marker = '.', c = y_train)\n",
        "      # plt.show()\n",
        "\n",
        "      print(\"[Testing]: Count of test data after Undersampling = \", X_train.shape[0])\n",
        "\n",
        "    # Model Fitting & Predictions on test dataset\n",
        "    y_pred = model(X_train, y_train, X_test)\n",
        "\n",
        "    # Evaluation Metrics\n",
        "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "    precision, recall, F1_score, _ = metrics.precision_recall_fscore_support(y_test, y_pred, beta = 1.0, average = 'macro')\n",
        "    metrics_list.append([accuracy, precision, recall, F1_score])\n",
        "\n",
        "    iterations += 1\n",
        "  \n",
        "  metrics_list = np.mean(metrics_list, axis = 0)\n",
        "\n",
        "  print(\"\\n---------------  Cross-validated Evaluation Metrics  ---------------\\n\")\n",
        "  print(\"Accuracy \\t= \\t\", metrics_list[0])\n",
        "  print(\"Precision \\t= \\t\", metrics_list[1])\n",
        "  print(\"Recall \\t\\t= \\t\", metrics_list[2])\n",
        "  print(\"F1 score \\t= \\t\", 2 * metrics_list[1] * metrics_list[2] / (metrics_list[1] + metrics_list[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smBuW5yTz-rS"
      },
      "source": [
        "## 1. Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPfoGsJSBBlK"
      },
      "source": [
        "def logistic_regression(X_train, y_train, X_test):\n",
        "  print(\"[Executing]: Running Logistic Regression model ...\\n\")\n",
        "\n",
        "  # Model Fitting\n",
        "  model = LogisticRegression(random_state = 0, solver='lbfgs', multi_class='multinomial')\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # Predictions on test dataset\n",
        "  y_pred = model.predict(X_test)\n",
        "  \n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VL38GYP0HOR"
      },
      "source": [
        "##2. AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV1zmR230J2J"
      },
      "source": [
        "# Decision stump used as weak classifier\n",
        "class DecisionStump:\n",
        "  def __init__(self):\n",
        "    self.polarity = 1\n",
        "    self.feature_idx = None\n",
        "    self.threshold = None\n",
        "    self.alpha = None\n",
        "\n",
        "  def predict(self, X):\n",
        "    n_samples = X.shape[0]\n",
        "    X_column = X[:, self.feature_idx]\n",
        "    predictions = np.ones(n_samples)\n",
        "    if self.polarity == 1:\n",
        "      predictions[X_column < self.threshold] = -1\n",
        "    else:\n",
        "      predictions[X_column > self.threshold] = -1\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "class Adaboost:\n",
        "  def __init__(self, n_clf=5):\n",
        "    self.n_clf = n_clf\n",
        "    self.clfs = []\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    n_samples, n_features = X.shape\n",
        "\n",
        "    # Initialize weights to 1/N\n",
        "    w = np.full(n_samples, (1 / n_samples))\n",
        "\n",
        "    self.clfs = []\n",
        "\n",
        "    # Iterate through classifiers\n",
        "    for _ in range(self.n_clf):\n",
        "      clf = DecisionStump()\n",
        "      min_error = float(\"inf\")\n",
        "\n",
        "      # greedy search to find best threshold and feature\n",
        "      for feature_i in range(n_features):\n",
        "        X_column = X[:, feature_i]\n",
        "        thresholds = np.unique(X_column)\n",
        "\n",
        "        for threshold in thresholds:\n",
        "          # predict with polarity 1\n",
        "          p = 1\n",
        "          predictions = np.ones(n_samples)\n",
        "          predictions[X_column < threshold] = -1\n",
        "\n",
        "          # Error = sum of weights of misclassified samples\n",
        "          misclassified = w[y != predictions]\n",
        "          error = sum(misclassified)\n",
        "\n",
        "          if error > 0.5:\n",
        "            error = 1 - error\n",
        "            p = -1\n",
        "\n",
        "          # store the best configuration\n",
        "          if error < min_error:\n",
        "            clf.polarity = p\n",
        "            clf.threshold = threshold\n",
        "            clf.feature_idx = feature_i\n",
        "            min_error = error\n",
        "\n",
        "      # calculate alpha\n",
        "      EPS = 1e-10\n",
        "      clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n",
        "\n",
        "      # calculate predictions and update weights\n",
        "      predictions = clf.predict(X)\n",
        "\n",
        "      w *= np.exp(-clf.alpha * y * predictions)\n",
        "      # Normalize to one\n",
        "      w /= np.sum(w)\n",
        "\n",
        "      # Save classifier\n",
        "      self.clfs.append(clf)\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\n",
        "    y_pred = np.sum(clf_preds, axis=0)\n",
        "    y_pred = np.sign(y_pred)\n",
        "\n",
        "    return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFJ_7Srrlak0"
      },
      "source": [
        "def adaboost_classifier(X_train, y_train, X_test):\n",
        "  print(\"[Executing]: Running Adaboost ...\\n\")\n",
        "\n",
        "  # Model Fitting\n",
        "  # model = Adaboost()\n",
        "  model = AdaBoostClassifier()\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # Predictions on test dataset\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaLHOLZa0LTJ"
      },
      "source": [
        "## 3. COBRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN_QF73a0PiS"
      },
      "source": [
        "**References**\n",
        "\n",
        "\n",
        "1.   https://github.com/bhargavvader/pycobra\n",
        "2.   https://github.com/bhargavvader/personal/tree/master/notebooks/pycobra\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDcdGSyMeAKL"
      },
      "source": [
        "### a. In-built"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkLr2HWK0W9r"
      },
      "source": [
        "def cobra_classifier(X_train, y_train, X_test):\n",
        "  print(\"[Executing]: Running Cobra Model ...\\n\")\n",
        "\n",
        "  # Model Fitting\n",
        "  model = ClassifierCobra(machine_list = 'basic')  \n",
        "  # advanced = ['tree', 'knn', 'svm', 'logreg', 'naive_bayes', 'lda', 'neural_network']\n",
        "  # basic = ['sgd', 'tree', 'knn', 'svm']\n",
        "  \n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # Predictions on test dataset\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKb_Q0aZeDF0"
      },
      "source": [
        "###b. Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emYIuMzjeEk-"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
        "from sklearn import neighbors, tree, svm\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CobraClassifier:\n",
        "    def __init__(self, seed=42, epsilon=0.001, threshold=0.5, machines=None):\n",
        "        self.seed = seed\n",
        "        self.epsilon = epsilon\n",
        "        self.threshold = threshold\n",
        "        self.machines = machines\n",
        "\n",
        "\n",
        "    def fit(self, X, y, X_k=None, y_k=None, X_l=None, y_l=None, flag=False):\n",
        "        # if flag == True => X_k values are defined\n",
        "        self.X, self.y = X, y\n",
        "        self.X_k, self.y_k = X_k, y_k\n",
        "        self.X_l, self.y_l = X_l, y_l\n",
        "        \n",
        "        self.machine_estimators = {}\n",
        "\n",
        "        if flag == False:\n",
        "            self.generate_training_data()\n",
        "\n",
        "        # Train machine estimators on the training data (D_k)\n",
        "        self.setup_machines()\n",
        "        self.execute_machines()\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def predict_helper(self, X, alpha):\n",
        "        res = {}\n",
        "        for m in self.machines:\n",
        "            predicted_label = self.machine_estimators[m].predict(X)\n",
        "            res[m] = {}\n",
        "\n",
        "            for idx in range(len(self.X_l)):\n",
        "                if math.fabs(self.machine_predictions[m][idx] - predicted_label) <= self.epsilon:\n",
        "                    res[m][idx] = 1\n",
        "                else:\n",
        "                    res[m][idx] = 0\n",
        "        \n",
        "        filtered_points = []\n",
        "        for idx in range(0, len(self.X_l)):\n",
        "            sum = 0\n",
        "            for m in res:\n",
        "                if res[m][idx] == 1:\n",
        "                    sum += 1\n",
        "                if sum >= alpha:\n",
        "                    filtered_points.append(idx)\n",
        "                    break\n",
        "\n",
        "        if len(filtered_points) == 0:\n",
        "            return 0\n",
        "\n",
        "        score = 0\n",
        "        for idx in filtered_points:\n",
        "            score += self.y_l[idx]\n",
        "        score = score / len(filtered_points)\n",
        "        \n",
        "        final_label = 1 if score >= self.threshold else 0\n",
        "        return final_label\n",
        "\n",
        "\n",
        "    def predict(self, X, alpha=None):\n",
        "        n = len(X)\n",
        "        predicted_labels = np.zeros(n)\n",
        "        \n",
        "        if alpha is None:\n",
        "            alpha = len(self.machines)\n",
        "\n",
        "        for i in range(n):\n",
        "            predicted_labels[i] = self.predict_helper(X[i].reshape(1, -1), alpha)\n",
        "        \n",
        "        return predicted_labels\n",
        "\n",
        "\n",
        "    def set_epsilon():\n",
        "        pass\n",
        "\n",
        "\n",
        "    def setup_machines(self):\n",
        "        for m in self.machines:\n",
        "            if m == 'knn':\n",
        "                self.machine_estimators[m] = neighbors.KNeighborsClassifier().fit(self.X_k, self.y_k)\n",
        "            elif m == 'random_forest':\n",
        "                self.machine_estimators[m] = RandomForestClassifier(random_state=self.seed).fit(self.X_k, self.y_k)\n",
        "            elif m == 'logistic_regression':\n",
        "                self.machine_estimators[m] = LogisticRegression(random_state=self.seed).fit(self.X_k, self.y_k)\n",
        "            elif m == 'svm':\n",
        "                self.machine_estimators[m] = svm.SVC().fit(self.X_k, self.y_k)\n",
        "            elif m == 'decision_trees':\n",
        "                self.machine_estimators[m] = tree.DecisionTreeClassifier().fit(self.X_k, self.y_k)\n",
        "            elif m == 'naive_bayes':\n",
        "                self.machine_estimators[m] = GaussianNB().fit(self.X_k, self.y_k)\n",
        "            elif m == 'stochastic_gradient_decision':\n",
        "                self.machine_estimators[m] = SGDClassifier().fit(self.X_k, self.y_k)\n",
        "            elif m == 'ridge':\n",
        "                self.machine_estimators[m] = RidgeClassifier().fit(self.X_k, self.y_k)\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def execute_machines(self):\n",
        "        self.machine_predictions = {}\n",
        "  \n",
        "        for m in self.machines:\n",
        "            self.machine_predictions[m] = self.machine_estimators[m].predict(self.X_l)\n",
        "\n",
        "        return self\n",
        "    \n",
        "\n",
        "    def generate_training_data(self, k=None, l=None):\n",
        "        \"\"\" \n",
        "        Splits the data into training (D_k) and testing part (D_l) for execution of models as specified in the COBRA paper\n",
        "        \"\"\"\n",
        "\n",
        "        if k is None or l is None:\n",
        "            n = len(self.X)\n",
        "            k = int(3*n/4)\n",
        "            l = int(n/4)\n",
        "\n",
        "        self.X_k, self.y_k = self.X[ : k], self.y[ : k]\n",
        "        self.X_l, self.y_l = self.X[k : ], self.y[k : ]\n",
        "        \n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AywYUx1p12S8"
      },
      "source": [
        "def cobra_classifier_scratch(X_train, y_train, X_test):\n",
        "  print(\"[Executing]: Running Cobra Model ...\\n\")\n",
        "\n",
        "  # Model Fitting\n",
        "  model = CobraClassifier(machines = ['knn', 'logistic_regression', 'svm', 'naive_bayes', 'ridge'])  \n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # Predictions on test dataset\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdf8qczIs-C6"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prxTpxRbIOVH"
      },
      "source": [
        "def prepare_data(seed, choice=1):\n",
        "  if choice == 1:\n",
        "    \"\"\" Dataset - 1 \"\"\"\n",
        "    # Ref - https://scikit-learn.org/stable/auto_examples/datasets/plot_random_dataset.html#sphx-glr-auto-examples-datasets-plot-random-dataset-py\n",
        "    X, y = datasets.make_classification(n_samples = 1000, n_classes = 2, weights = [0.2, 0.8], class_sep = 0.9, \n",
        "                                  n_features = 5, n_redundant = 1, n_informative = 3, n_clusters_per_class = 1, random_state = seed)\n",
        "\n",
        "  elif choice == 2:\n",
        "    \"\"\" Dataset - 2 \"\"\"\n",
        "    dataset = pd.read_csv('./winequality-red.csv', sep=';')\n",
        "\n",
        "    def reviews(row):\n",
        "      if row['quality'] > 7:\n",
        "          return 1\n",
        "      else: \n",
        "          return 0\n",
        "\n",
        "    dataset['reviews'] = dataset.apply(reviews, axis=1)\n",
        "    dataset['reviews'].value_counts()\n",
        "\n",
        "    features = list(dataset.columns)[:-1]\n",
        "    target = 'reviews'\n",
        "    X = np.asarray(dataset[features])\n",
        "    y = np.asarray(dataset[target])\n",
        "\n",
        "    majority_class_label = int(sum(y) > 0.5 * len(y))\n",
        "    print(\"[Testing]: Majority class = \", majority_class_label, \"\\tSum of Target variable = \", sum(y), \"\\tLength of Target variable = \", len(y))\n",
        "    # print(dataset.describe())\n",
        "\n",
        "  elif choice == 3:\n",
        "    \"\"\" Dataset - 3 \"\"\"\n",
        "    dataset = pd.read_csv('./winequality-white.csv', sep=';')\n",
        "\n",
        "    def reviews(row):\n",
        "      if row['quality'] > 7:\n",
        "          return 1\n",
        "      else: \n",
        "          return 0\n",
        "\n",
        "    dataset['reviews'] = dataset.apply(reviews, axis=1)\n",
        "    dataset['reviews'].value_counts()\n",
        "\n",
        "    features = list(dataset.columns)[:-1]\n",
        "    target = 'reviews'\n",
        "    X = np.asarray(dataset[features])\n",
        "    y = np.asarray(dataset[target])\n",
        "\n",
        "    majority_class_label = int(sum(y) > 0.5 * len(y))\n",
        "    print(\"[Testing]: Majority class = \", majority_class_label, \"\\tSum of Target variable = \", sum(y), \"\\tLength of Target variable = \", len(y))\n",
        "    # print(dataset.describe())\n",
        "\n",
        "  elif choice == 4:\n",
        "    \"\"\" Dataset - 4 \"\"\"\n",
        "    # Ref - https://archive.ics.uci.edu/ml/datasets/car+evaluation\n",
        "    dataset = pd.read_csv('./car.data', header=None, sep=',')\n",
        "    \n",
        "    \"\"\" Pre-processing \"\"\"\n",
        "    def create_target(row, val='vgood'):\n",
        "      if row[6] == val:\n",
        "          return 1\n",
        "      else: \n",
        "          return 0\n",
        "\n",
        "    def encode_attribute_1(row):\n",
        "      if row[0] == 'vhigh':   return 1\n",
        "      elif row[0] == 'high':  return 2\n",
        "      elif row[0] == 'med':   return 3\n",
        "      else:                   return 4\n",
        "    \n",
        "    def encode_attribute_2(row):\n",
        "      if row[1] == 'vhigh':   return 1\n",
        "      elif row[1] == 'high':  return 2\n",
        "      elif row[1] == 'med':   return 3\n",
        "      else:                   return 4\n",
        "\n",
        "    def encode_attribute_3(row):\n",
        "      if row[2] == '5more':   return 5\n",
        "      else:                   return row[2]\n",
        "\n",
        "    def encode_attribute_4(row):\n",
        "      if row[3] == 'more':    return 5\n",
        "      else:                   return row[3]\n",
        "\n",
        "    def encode_attribute_5(row):\n",
        "      if row[4] == 'small':   return 1\n",
        "      elif row[4] == 'med':   return 2\n",
        "      else:                   return 3\n",
        "\n",
        "    def encode_attribute_6(row):\n",
        "      if row[5] == 'low':     return 1\n",
        "      elif row[5] == 'med':   return 2\n",
        "      else:                   return 3\n",
        "\n",
        "    dataset[0] = dataset.apply(encode_attribute_1, axis=1)\n",
        "    dataset[1] = dataset.apply(encode_attribute_2, axis=1)\n",
        "    dataset[2] = dataset.apply(encode_attribute_3, axis=1)\n",
        "    dataset[3] = dataset.apply(encode_attribute_4, axis=1)\n",
        "    dataset[4] = dataset.apply(encode_attribute_5, axis=1)\n",
        "    dataset[5] = dataset.apply(encode_attribute_6, axis=1)\n",
        "    dataset['target'] = dataset.apply(create_target, axis=1)\n",
        "    \n",
        "    \"\"\"\"\"\"\n",
        "\n",
        "    features = list(dataset.columns)[:-2]\n",
        "    target = 'target'\n",
        "    X = np.asarray(dataset[features])\n",
        "    y = np.asarray(dataset[target])\n",
        "\n",
        "    majority_class_label = int(sum(y) > 0.5 * len(y))\n",
        "    print(\"[Testing]: Majority class = \", majority_class_label, \"\\tSum of Target variable = \", sum(y), \"\\tLength of Target variable = \", len(y))\n",
        "    # print(dataset.describe())\n",
        "    # print(dataset.head(10))\n",
        "\n",
        "  elif choice == 5:\n",
        "    \"\"\" Dataset - 5 \"\"\"\n",
        "    # Ref - https://archive.ics.uci.edu/ml/datasets/ecoli, https://www.kaggle.com/kannanaikkal/ecoli-uci-dataset\n",
        "    dataset = pd.read_csv('./ecoli.csv')\n",
        "    \n",
        "    def create_target(row, val='imU'):\n",
        "      if row['SITE'] == val:\n",
        "          return 1\n",
        "      else: \n",
        "          return 0\n",
        "\n",
        "    dataset = dataset.drop('SEQUENCE_NAME', 1)\n",
        "    # for s in set(dataset['SITE']):\n",
        "    #   print(s, \" = \", dataset['SITE'].str.count(s).sum())\n",
        "\n",
        "    dataset['target'] = dataset.apply(create_target, axis=1)\n",
        "    \n",
        "    features = list(dataset.columns)[:-2]\n",
        "    target = 'target'\n",
        "    X = np.asarray(dataset[features])\n",
        "    y = np.asarray(dataset[target])\n",
        "\n",
        "    majority_class_label = int(sum(y) > 0.5 * len(y))\n",
        "    print(\"[Testing]: Majority class = \", majority_class_label, \"\\tSum of Target variable = \", sum(y), \"\\tLength of Target variable = \", len(y))\n",
        "    # print(dataset.describe())\n",
        "    # print(dataset.head(10))\n",
        "\n",
        "  elif choice == 6:\n",
        "    \"\"\" Dataset - 6 \"\"\"\n",
        "    # Ref - https://archive.ics.uci.edu/ml/datasets/abalone\n",
        "    dataset = pd.read_csv('./abalone.data', header=None, sep=',')\n",
        "    \n",
        "    def create_target(row, val=20):\n",
        "      if row[8] >= val:\n",
        "          return 1\n",
        "      else: \n",
        "          return 0\n",
        "\n",
        "    def encode_attribute(row):\n",
        "      if row[0] == 'M':     return 0\n",
        "      elif row[0] == 'F':   return 1\n",
        "      else:                 return 2\n",
        "\n",
        "    dataset[0] = dataset.apply(encode_attribute, axis = 1)\n",
        "    dataset['target'] = dataset.apply(create_target, axis=1)\n",
        "    dataset['target'].value_counts()\n",
        "\n",
        "    features = list(dataset.columns)[:-2]\n",
        "    target = 'target'\n",
        "    X = np.asarray(dataset[features])\n",
        "    y = np.asarray(dataset[target])\n",
        "\n",
        "    majority_class_label = int(sum(y) > 0.5 * len(y))\n",
        "    print(\"[Testing]: Majority class = \", majority_class_label, \"\\tSum of Target variable = \", sum(y), \"\\tLength of Target variable = \", len(y))\n",
        "    # print(dataset.describe())\n",
        "    # print(dataset.head(10))\n",
        "    # print(dataset[8].value_counts())\n",
        "\n",
        "  elif choice == 7:\n",
        "    \"\"\" Dataset - 7 \"\"\"\n",
        "    # Ref - https://archive.ics.uci.edu/ml/datasets/nursery\n",
        "    dataset = pd.read_csv('./nursery.data', header=None, sep=',')\n",
        "    \n",
        "    def create_target(row, val='very_recom'):\n",
        "      if row[8] == val:\n",
        "          return 1\n",
        "      else: \n",
        "          return 0\n",
        "\n",
        "    def encode_attributes(original_dataframe, feature_to_encode):\n",
        "      dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
        "      res = pd.concat([original_dataframe, dummies], axis=1)\n",
        "      res = res.drop([feature_to_encode], axis=1)\n",
        "      return res\n",
        "\n",
        "    features_to_encode = [0, 1, 2, 3, 4, 5, 6, 7]\n",
        "    for feature in features_to_encode:\n",
        "        dataset = encode_attributes(dataset, feature)\n",
        "\n",
        "    dataset['target'] = dataset.apply(create_target, axis=1)\n",
        "\n",
        "    features = list(dataset.columns)[1:-1]\n",
        "    target = 'target'\n",
        "    X = np.asarray(dataset[features])\n",
        "    y = np.asarray(dataset[target])\n",
        "\n",
        "    majority_class_label = int(sum(y) > 0.5 * len(y))\n",
        "    print(\"[Testing]: Majority class = \", majority_class_label, \"\\tSum of Target variable = \", sum(y), \"\\tLength of Target variable = \", len(y))\n",
        "    # print(dataset.describe())\n",
        "    # print(dataset.head(10))\n",
        "    # for s in set(dataset[8]):\n",
        "    #   print(s, \" = \", dataset[8].str.count(s).sum())\n",
        "\n",
        "  else:\n",
        "    print(\"No dataset available\")\n",
        "\n",
        "  # plt.scatter(X[:, 0], X[:, 1], marker = '.', c = y)\n",
        "  # plt.show()\n",
        "  # assert np.any(np.isnan(dataset)) == False\n",
        "  \n",
        "  return X, y, majority_class_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7_LCD6kcf3j"
      },
      "source": [
        "# Driver Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o39JsH7BHiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45bb8fae-f73b-4526-b68f-0c5e9d934dde"
      },
      "source": [
        "X, y, majority_class_label = prepare_data(32, choice=6)\n",
        "num_splits, seed = 2, 32\n",
        "\n",
        "# models = [cobra_classifier_scratch]\n",
        "\n",
        "# for m in models:\n",
        "#   print(\"\\n\\n#############################  MODEL -\", m.__name__, \"  #############################\")\n",
        "#   print(\"\\n=======================  Executing without undersampling  =======================\")\n",
        "#   execute_model(X, y, num_splits, seed, m)\n",
        "\n",
        "#   print(\"\\n\\n=======================  Executing with undersampling  =======================\")\n",
        "#   execute_model(X, y, num_splits, seed, m, with_undersampling = True, majority_class = majority_class_label, undersampling_method = near_miss_v1)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Testing]: Majority class =  0 \tSum of Target variable =  62 \tLength of Target variable =  4177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1jOtRq2hNrP",
        "outputId": "4bfc9f18-faf8-4b8a-82e7-022b8cd3df91"
      },
      "source": [
        "X, y, majority_class_label = prepare_data(32, choice=5)\n",
        "num_splits, seed = 2, 32\n",
        "\n",
        "models = [logistic_regression, adaboost_classifier, cobra_classifier_scratch]\n",
        "\n",
        "for m in models:\n",
        "  print(\"\\n\\n#############################  MODEL -\", m.__name__, \"  #############################\")\n",
        "  execute_model(X, y, num_splits, seed, m)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Testing]: Majority class =  0 \tSum of Target variable =  35 \tLength of Target variable =  336\n",
            "\n",
            "\n",
            "#############################  MODEL - logistic_regression   #############################\n",
            "\n",
            "****************  Executing iteration - 1 of KFold Data split  ****************\n",
            "[Executing]: Running Logistic Regression model ...\n",
            "\n",
            "\n",
            "****************  Executing iteration - 2 of KFold Data split  ****************\n",
            "[Executing]: Running Logistic Regression model ...\n",
            "\n",
            "\n",
            "---------------  Cross-validated Evaluation Metrics  ---------------\n",
            "\n",
            "Accuracy \t= \t 0.9226190476190477\n",
            "Precision \t= \t 0.8058383609854198\n",
            "Recall \t\t= \t 0.7415800978227935\n",
            "F1 score \t= \t 0.7723750316758925\n",
            "\n",
            "\n",
            "#############################  MODEL - adaboost_classifier   #############################\n",
            "\n",
            "****************  Executing iteration - 1 of KFold Data split  ****************\n",
            "[Executing]: Running Adaboost ...\n",
            "\n",
            "\n",
            "****************  Executing iteration - 2 of KFold Data split  ****************\n",
            "[Executing]: Running Adaboost ...\n",
            "\n",
            "\n",
            "---------------  Cross-validated Evaluation Metrics  ---------------\n",
            "\n",
            "Accuracy \t= \t 0.8779761904761905\n",
            "Precision \t= \t 0.6792417417417418\n",
            "Recall \t\t= \t 0.6922239535990997\n",
            "F1 score \t= \t 0.6856714032517663\n",
            "\n",
            "\n",
            "#############################  MODEL - cobra_classifier_scratch   #############################\n",
            "\n",
            "****************  Executing iteration - 1 of KFold Data split  ****************\n",
            "[Executing]: Running Cobra Model ...\n",
            "\n",
            "\n",
            "****************  Executing iteration - 2 of KFold Data split  ****************\n",
            "[Executing]: Running Cobra Model ...\n",
            "\n",
            "\n",
            "---------------  Cross-validated Evaluation Metrics  ---------------\n",
            "\n",
            "Accuracy \t= \t 0.8720238095238095\n",
            "Precision \t= \t 0.5409226190476191\n",
            "Recall \t\t= \t 0.5911569925983639\n",
            "F1 score \t= \t 0.5649252674724745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}